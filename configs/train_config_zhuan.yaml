# OmniSVG Training Configuration - LOW MEMORY VERSION
# This configuration reduces memory usage for training on GPUs with limited VRAM

# ==============================================================================
# Model Configuration
# ==============================================================================
model:
  size: "4B"
  use_flash_attn: true
  torch_dtype: "bfloat16"
  
  # Gradient Checkpointing 设置
  # 注意：模型层面的 gradient_checkpointing 与 FSDP 在 PyTorch 2.5.0 有冲突
  # 解决方案：关闭模型层面的，使用 FSDP 原生的 activation_checkpointing
  # 在 fsdp_config_transformer.yaml 中已设置 fsdp_activation_checkpointing: true
  # 这样既能节省显存（~30-40%），又不会与 FSDP 冲突
  use_gradient_checkpointing: false

# ==============================================================================
# Data Configuration  
# ==============================================================================
data:
  data_dir: "/data/phd23_weiguang_zhang/works/svg/my_zhuan2"
  
  # Reduced image size: 448 -> 336 (saves ~40% image memory)
  target_image_size: 224
  
  # Reduced sequence length: 2048 -> 1536 (saves ~25% sequence memory)
  max_seq_length: 2048
  text_max_length: 800
  
  text_source_probabilities:
    detail_description: 0.60
    brief_description: 0.40

# ==============================================================================
# Training Configuration
# ==============================================================================
training:
  learning_rate: 1.0e-4 # 1.0e-5
  weight_decay: 0.1 # 0.015
  max_grad_norm: 1.0 # 0.8
  
  epochs: 15
  # Increased gradient accumulation: 4 -> 8
  # This maintains effective batch size while using less memory per step
  gradient_accumulation_steps: 4
  
  scheduler:
    type: "cosine"
    warmup_steps: 500 # 50000
    num_cycles: 0.5
  
  task_balance:
    initial_text_only_ratio: 0.0
    # 0.5：平衡训练（推荐）
    # 0.7：偏重文本理解
    # 1.0：纯文本到SVG（显存友好）
    # 0.0：纯图像到SVG（需要大量显存）
  
  loss_weights:
    text_task: 0 # 1.5
    image_task: 1.0

# ==============================================================================
# Logging and Checkpointing
# ==============================================================================
logging:
  log_every: 15 # 10
  save_every: 2000 # 3000
  val_every: 1000 # 5000

# ==============================================================================
# DataLoader Configuration
# ==============================================================================
dataloader:
  num_workers: 4  # Reduced from 8 to save CPU memory
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2

# ==============================================================================
# Random Seed
# ==============================================================================
seed: 2023
