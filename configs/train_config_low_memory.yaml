# OmniSVG Training Configuration - LOW MEMORY VERSION
# This configuration reduces memory usage for training on GPUs with limited VRAM

# ==============================================================================
# Model Configuration
# ==============================================================================
model:
  size: "4B"
  use_flash_attn: true
  torch_dtype: "bfloat16"
  
  # Enable gradient checkpointing to save ~30-40% memory
  # Trade-off: ~20% slower training
  use_gradient_checkpointing: true

# ==============================================================================
# Data Configuration  
# ==============================================================================
data:
  data_dir: "/data/phd23_weiguang_zhang/works/svg/MMSVG-icon-sample"
  
  # Reduced image size: 448 -> 336 (saves ~40% image memory)
  target_image_size: 336
  
  # Reduced sequence length: 2048 -> 1536 (saves ~25% sequence memory)
  max_seq_length: 1536
  text_max_length: 800
  
  text_source_probabilities:
    detail_description: 0.60
    brief_description: 0.40

# ==============================================================================
# Training Configuration
# ==============================================================================
training:
  learning_rate: 1.0e-5
  weight_decay: 0.015
  max_grad_norm: 0.8
  
  epochs: 150
  # Increased gradient accumulation: 4 -> 8
  # This maintains effective batch size while using less memory per step
  gradient_accumulation_steps: 8
  
  scheduler:
    type: "cosine"
    warmup_steps: 50000
    num_cycles: 0.5
  
  task_balance:
    initial_text_only_ratio: 0.50
  
  loss_weights:
    text_task: 1.5
    image_task: 1.0

# ==============================================================================
# Logging and Checkpointing
# ==============================================================================
logging:
  log_every: 10
  save_every: 3000
  val_every: 5000

# ==============================================================================
# DataLoader Configuration
# ==============================================================================
dataloader:
  num_workers: 4  # Reduced from 8 to save CPU memory
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2

# ==============================================================================
# Random Seed
# ==============================================================================
seed: 2023
